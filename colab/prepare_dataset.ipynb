{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9870b73a-1708-48d3-8dba-ac1286c37c1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T09:40:03.661009Z",
     "iopub.status.busy": "2023-05-02T09:40:03.660166Z",
     "iopub.status.idle": "2023-05-02T09:40:03.716364Z",
     "shell.execute_reply": "2023-05-02T09:40:03.712312Z",
     "shell.execute_reply.started": "2023-05-02T09:40:03.660946Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import gzip\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from xml.etree.ElementTree import parse\n",
    "from sys import getsizeof\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def merge_dataset(dataset_directories: list[str], result_dir:str):\n",
    "    datasets = glob.glob(f\"{dataset_directories}/*.gz\")\n",
    "    assert len(datasets) != 0, \"Please check that the dataset file exists.\"\n",
    "    result = []\n",
    "\n",
    "    pbar = tqdm.tqdm(\n",
    "        datasets,\n",
    "        smoothing=0,\n",
    "        leave=True,\n",
    "        dynamic_ncols=True,\n",
    "    )\n",
    "    for dataset_dir in pbar:\n",
    "        f = gzip.GzipFile(dataset_dir, \"r\")\n",
    "        temp_tokens = np.load(f)\n",
    "        result = np.concatenate((temp_tokens, result), axis=0) if len(result) != 0 else temp_tokens \n",
    "    \n",
    "    print(result.shape)\n",
    "    with gzip.open(result_dir, \"wb\") as f:\n",
    "        np.save(f, result)\n",
    "\n",
    "\n",
    "def encode_from_texts(texts:list[str], tokenizer: AutoTokenizer, block_size:int, BOS_TOKEN:str=\"[BOS]\", EOS_TOKEN:str=\"[EOS]\"):\n",
    "    tokens = []\n",
    "    pbar = tqdm.tqdm(\n",
    "        texts,\n",
    "        smoothing=0,\n",
    "        leave=True,\n",
    "        dynamic_ncols=True,\n",
    "    )\n",
    "    for text in pbar:\n",
    "        # print(text)\n",
    "        if text == \"\":\n",
    "            continue\n",
    "        \n",
    "        # text = text.replace(\"\\n\", f\"{SEP_TOKEN}\")\n",
    "        text = f\"{BOS_TOKEN} {text} {EOS_TOKEN}\" \n",
    "\n",
    "        temp_tokens = np.array(tokenizer.encode(text), dtype=np.int64)\n",
    "        length = len(temp_tokens)\n",
    "        padding = -length % (block_size+1)\n",
    "        temp_tokens = np.reshape(np.concatenate((temp_tokens, np.ones(padding)*tokenizer.encode(\"[PAD]\"))), (-1, block_size+1))\n",
    "        # print(temp_tokens.shape)\n",
    "        tokens = np.concatenate((tokens, temp_tokens), axis=0) if len(tokens) != 0 else temp_tokens\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def read_text_from_xml(xml_dir:str):\n",
    "    try:\n",
    "        tree = parse(xml_dir)\n",
    "        root = tree.getroot()\n",
    "        text = \" \".join([x.text for x in root.findall(\"text\")[0].findall(\"p\")])\n",
    "        return text\n",
    "    except: return ''\n",
    "\n",
    "def encode_text_from_xml(folder_dir: str, tokenizer: AutoTokenizer, block_size:int, BOS_TOKEN:str, EOS_TOKEN:str):\n",
    "    assert folder_dir[-1] != \"/\", \"Check the directory please.\"\n",
    "    xml_file_directories = glob.glob(f\"{folder_dir}/*\")\n",
    "\n",
    "    texts = [read_text_from_xml(xml_dir) for xml_dir in xml_file_directories]\n",
    "    \n",
    "    tokens = encode_from_texts(texts, tokenizer, block_size, BOS_TOKEN, EOS_TOKEN)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def read_text_from_txt(txt_dir: str, encoding):\n",
    "    with open(txt_dir, \"r\", encoding=encoding) as f:\n",
    "        texts = f.read()\n",
    "    print(texts[:100])\n",
    "    return texts\n",
    "\n",
    "def encode_text_from_txt(folder_dir: str, tokenizer: AutoTokenizer, block_size: int, encoding):\n",
    "    assert folder_dir[-1] != \"/\", \"Check the directory please.\"\n",
    "    txt_file_directories = glob.glob(f\"{folder_dir}/*.txt\")\n",
    "\n",
    "    texts = [read_text_from_txt(txt_dir, encoding=encoding) for txt_dir in txt_file_directories]\n",
    "    \n",
    "    tokens = encode_from_texts(texts, tokenizer, block_size)\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c679a05a-9f3a-49a3-8c55-705f05c2bbbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T06:27:15.590825Z",
     "iopub.status.busy": "2023-05-02T06:27:15.589841Z",
     "iopub.status.idle": "2023-05-02T06:27:17.382760Z",
     "shell.execute_reply": "2023-05-02T06:27:17.381673Z",
     "shell.execute_reply.started": "2023-05-02T06:27:15.590788Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',\n",
    "    bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d5a96df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로그인 무림\n",
      "\n",
      "프롤로그\n",
      "\n",
      "러시아 속담에 이런 말이 있다.\n",
      "\n",
      "'공짜 치즈는 쥐덫 위에 있다.'\n",
      "\n",
      "지금 생각해 보면 그날의 모든 것이 누군가의 쥐덫이 아닌가 싶다.\n",
      "\n",
      "7년간 일했던 직\n"
     ]
    }
   ],
   "source": [
    "text = read_text_from_txt(\"../dataset/korean_murim_book.txt\", encoding=\"cp949\")\n",
    "temp_tokens = np.array(tokenizer.encode(text), dtype=np.int64)\n",
    "length = len(temp_tokens)\n",
    "padding = -length % (64+1)\n",
    "temp_tokens = np.reshape(np.concatenate((temp_tokens, np.ones(padding)*tokenizer.encode(\"[PAD]\"))), (-1, 64+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20af5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"../tmp/murim.tar.gz\", \"wb\") as f:\n",
    "    np.save(f, temp_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74ecbbef-7adc-4466-931d-4099a661935a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T06:33:56.989654Z",
     "iopub.status.busy": "2023-05-02T06:33:56.988871Z",
     "iopub.status.idle": "2023-05-02T06:37:30.464670Z",
     "shell.execute_reply": "2023-05-02T06:37:30.462810Z",
     "shell.execute_reply.started": "2023-05-02T06:33:56.989591Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset1 = encode_text_from_xml(\"./NIKL_NP_v1.2/국립국어원 비출판물 말뭉치(버전 1.2)\", tokenizer=tokenizer, block_size=128, BOS_TOKEN=\"[BOS]\", EOS_TOKEN=\"[EOS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5d92909-69f7-4277-ab70-bf7c1e67de76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T06:38:54.993977Z",
     "iopub.status.busy": "2023-05-02T06:38:54.993467Z",
     "iopub.status.idle": "2023-05-02T06:39:36.479361Z",
     "shell.execute_reply": "2023-05-02T06:39:36.478260Z",
     "shell.execute_reply.started": "2023-05-02T06:38:54.993937Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with gzip.open(\"dataset_cache.tar.gz\", \"wb\") as f:\n",
    "    np.save(f, dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1c6276a-530c-4bc7-aae7-9b8c72eeaaca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T06:45:49.021113Z",
     "iopub.status.busy": "2023-05-02T06:45:49.020473Z",
     "iopub.status.idle": "2023-05-02T06:45:49.033815Z",
     "shell.execute_reply": "2023-05-02T06:45:49.031401Z",
     "shell.execute_reply.started": "2023-05-02T06:45:49.021067Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs_to_process = glob.glob(\"./030.웹데이터 기반 한국어 말뭉치 데이터/01.데이터/1.Training/라벨링데이터/TL1/*\")\n",
    "\n",
    "dirs_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71d80677-6d1a-4a72-8d27-c18b590c6865",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T06:47:16.501626Z",
     "iopub.status.busy": "2023-05-02T06:47:16.500514Z",
     "iopub.status.idle": "2023-05-02T06:47:16.527180Z",
     "shell.execute_reply": "2023-05-02T06:47:16.525779Z",
     "shell.execute_reply.started": "2023-05-02T06:47:16.501583Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m directory \u001b[39m=\u001b[39m dirs_to_process[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m      2\u001b[0m files_to_process \u001b[39m=\u001b[39m glob\u001b[39m.\u001b[39mglob(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m/*.json\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "directory = dirs_to_process[0]\n",
    "files_to_process = glob.glob(f\"{directory}/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ceb99-bfbf-4820-b545-ecbcf91d2b4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T06:48:44.380564Z",
     "iopub.status.busy": "2023-05-02T06:48:44.380165Z",
     "iopub.status.idle": "2023-05-02T06:48:44.390629Z",
     "shell.execute_reply": "2023-05-02T06:48:44.389764Z",
     "shell.execute_reply.started": "2023-05-02T06:48:44.380533Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"./test.json\"\n",
    "\n",
    "with open(files_to_process[0], 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715fd4cf-5bd8-46ec-938f-6bd993285163",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T06:58:36.656181Z",
     "iopub.status.busy": "2023-05-02T06:58:36.655678Z",
     "iopub.status.idle": "2023-05-02T06:58:36.667676Z",
     "shell.execute_reply": "2023-05-02T06:58:36.665392Z",
     "shell.execute_reply.started": "2023-05-02T06:58:36.656145Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d', 'd']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"【워싱턴=신화/뉴시스】(이름) 기자 = 도널드 트럼프 미 대통령은 오는 5\".index(\"(이름) 기자\")\n",
    "\"【워싱턴=신화/뉴시스】(이름) 기자 = 도널드 트럼프 미 대통령은 오는 5\"[:-2]\n",
    "[\"d\", \"d\", \"k\", \"a\"][:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed77f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51830"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs_to_process = glob.glob(\"../../dataset/030.웹데이터 기반 한국어 말뭉치 데이터/TL1/*/*.json\")\n",
    "len(dirs_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4879cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51830/51830 [07:46<00:00, 111.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "sources = []\n",
    "\n",
    "total = 0\n",
    "\n",
    "def load_dataset(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for entity in data[\"named_entity\"]:\n",
    "        paragraph = []\n",
    "        for content in entity[\"content\"][:-2]:\n",
    "            sentence = content[\"sentence\"]\n",
    "\n",
    "            if \"(이름) 기자\" in sentence:\n",
    "                pos = sentence.index(\"(이름) 기자\") + 10\n",
    "                sentence = sentence[pos:]\n",
    "            ignore_signs = [\"참조링크\", \"관련기사\"]\n",
    "            for sign in ignore_signs: \n",
    "                if sign in sentence:\n",
    "                    sentence = \"\"\n",
    "                    break\n",
    "            paragraph.append(sentence)\n",
    "            \n",
    "        sources.append(\" \".join(paragraph))\n",
    "        del paragraph\n",
    "    \n",
    "import tqdm\n",
    "for filepath in tqdm.tqdm(dirs_to_process):\n",
    "    load_dataset(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b527dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../tmp/corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(\"\\n\\n====\\n\\n\".join(sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b32151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3262903551"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../tmp/corpus.txt\", \"r\") as f:\n",
    "    source = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "024567e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YSH\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from sentencepiece import SentencePieceProcessor, SentencePieceTrainer\n",
    "\n",
    "# https://github.com/Lightning-AI/lit-llama/blob/main/lit_llama/tokenizer.py\n",
    "class Tokenizer:\n",
    "    \"\"\"Tokenizer for LLaMA.\"\"\"\n",
    "    def __init__(self, model_path: Path) -> None:\n",
    "        self.processor = SentencePieceProcessor(model_file=str(model_path))\n",
    "        self.bos_id = self.processor.bos_id()\n",
    "        self.eos_id = self.processor.eos_id()\n",
    "        self.pad_id = self.processor.pad_id()\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return self.processor.vocab_size()\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        string: str,\n",
    "        bos: bool = True,\n",
    "        eos: bool = False,\n",
    "        max_length: int = -1,\n",
    "        pad: bool = False,\n",
    "        device: Optional[torch.device] = None\n",
    "    ) -> torch.Tensor:\n",
    "        tokens = self.processor.encode(string)\n",
    "        if bos:\n",
    "            tokens = [self.bos_id] + tokens\n",
    "        if eos:\n",
    "            tokens = tokens + [self.eos_id]\n",
    "        if max_length > 0:\n",
    "            tokens = tokens[:max_length]\n",
    "        if pad and len(tokens) < max_length:\n",
    "            tokens += [self.pad_id] * (max_length - len(tokens))\n",
    "\n",
    "        return torch.tensor(tokens, dtype=torch.int, device=device)\n",
    "\n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        return self.processor.decode(tokens.tolist())\n",
    "\n",
    "    @staticmethod\n",
    "    def train(input: str, destination: str, vocab_size=32000) -> None:\n",
    "        model_prefix = os.path.join(destination, \"tokenizer\")\n",
    "        SentencePieceTrainer.Train(input=input, model_prefix=model_prefix, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "029a05cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "destination_path = \"../tmp/tokenizer\"\n",
    "os.makedirs(destination_path, exist_ok=True)\n",
    "Tokenizer.train(input=\"../tmp/corpus.txt\", destination=destination_path, vocab_size=480000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1959492",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = source.split(\"\\n\\n====\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "594988a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4862715"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a6784c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(source)\n",
    "n = len(source)\n",
    "train_data = source[:int(n*0.9)]\n",
    "val_data = source[int(n*0.9):]\n",
    "del source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eee3517c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 486272/486272 [14:55:21<00:00,  9.05it/s]  \n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(\"../tmp/tokenizer/tokenizer.model\")\n",
    "\n",
    "import tqdm \n",
    "import numpy as np\n",
    "def encode_from_texts_v2(texts:list[str], tokenizer: Tokenizer, block_size:int):\n",
    "    tokens = []\n",
    "    pbar = tqdm.tqdm(\n",
    "        texts,\n",
    "        smoothing=0,\n",
    "        leave=True,\n",
    "        dynamic_ncols=True,\n",
    "    )\n",
    "    for text in pbar:\n",
    "        # print(text)\n",
    "        if text == \"\":\n",
    "            continue\n",
    "        \n",
    "        encoded_text = tokenizer.encode(text, bos=True, eos=True)\n",
    "        temp_tokens = np.array(encoded_text, dtype=np.int64)\n",
    "        length = len(temp_tokens)\n",
    "        padding = -length % (block_size+1)\n",
    "        temp_tokens = np.reshape(np.concatenate((temp_tokens, np.ones(padding)*tokenizer.pad_id)), (-1, block_size+1))\n",
    "        tokens = np.concatenate((tokens, temp_tokens), axis=0) if len(tokens) != 0 else temp_tokens\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# train_ids = encode_from_texts_v2(train_data, tokenizer, block_size=256)\n",
    "val_ids = encode_from_texts_v2(val_data, tokenizer, block_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51f97169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd941a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "# with gzip.open(\"train_ids_dataset.tar.gz\", \"wb\") as f:\n",
    "#     np.save(f, train_ids)\n",
    "with gzip.open(\"large_dataset.tar.gz\", \"wb\") as f:\n",
    "    np.save(f, val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e0bc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_indices = val_ids == -1\n",
    "val_ids[target_indices] = 480001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1fcaaa7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca89f60-d64d-41a8-ba7b-19fc62421c3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T07:03:00.250163Z",
     "iopub.status.busy": "2023-05-02T07:03:00.249036Z",
     "iopub.status.idle": "2023-05-02T07:14:41.734510Z",
     "shell.execute_reply": "2023-05-02T07:14:41.726462Z",
     "shell.execute_reply.started": "2023-05-02T07:03:00.250116Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sources = []\n",
    "\n",
    "total = 0\n",
    "\n",
    "def load_dataset(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for entity in data[\"named_entity\"]:\n",
    "        paragraph = []\n",
    "        for content in entity[\"content\"][:-2]:\n",
    "            sentence = content[\"sentence\"]\n",
    "\n",
    "            if \"(이름) 기자\" in sentence:\n",
    "                pos = sentence.index(\"(이름) 기자\") + 10\n",
    "                sentence = sentence[pos:]\n",
    "            ignore_signs = [\"참조링크\", \"관련기사\"]\n",
    "            for sign in ignore_signs: \n",
    "                if sign in sentence:\n",
    "                    sentence = \"\"\n",
    "                    break\n",
    "            paragraph.append(sentence)\n",
    "            \n",
    "        sources.append(\" \".join(paragraph))\n",
    "        del paragraph\n",
    "    \n",
    "    total += 1\n",
    "    print(f\"{total} was processed\")\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "pool = multiprocessing.Pool(processes=8)\n",
    "pool.map(load_dataset, dirs_to_process)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d82368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4862715"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(\"\\n\\n====\\n\\n\".join(sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aca29af9-52ef-4bd6-a8f1-88bd12079db9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T07:14:41.755731Z",
     "iopub.status.busy": "2023-05-02T07:14:41.754144Z",
     "iopub.status.idle": "2023-05-02T07:27:41.233164Z",
     "shell.execute_reply": "2023-05-02T07:27:41.227616Z",
     "shell.execute_reply.started": "2023-05-02T07:14:41.755692Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▌                                                                                                                                                           | 17384/4862715 [12:59<60:19:44, 22.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mencode_from_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBOS_TOKEN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[BOS]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEOS_TOKEN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[EOS]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mencode_from_texts\u001b[0;34m(texts, tokenizer, block_size, BOS_TOKEN, EOS_TOKEN)\u001b[0m\n\u001b[1;32m     54\u001b[0m     temp_tokens \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(np\u001b[38;5;241m.\u001b[39mconcatenate((temp_tokens, np\u001b[38;5;241m.\u001b[39mones(padding)\u001b[38;5;241m*\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m))), (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, block_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# print(temp_tokens.shape)\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m temp_tokens\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokens = encode_from_texts(sources, tokenizer, block_size=128, BOS_TOKEN=\"[BOS]\", EOS_TOKEN=\"[EOS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a84f45c-a931-4f47-8061-9bc51790930f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T07:29:25.583060Z",
     "iopub.status.busy": "2023-05-02T07:29:25.582016Z",
     "iopub.status.idle": "2023-05-02T07:29:27.717261Z",
     "shell.execute_reply": "2023-05-02T07:29:27.714328Z",
     "shell.execute_reply.started": "2023-05-02T07:29:25.583021Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "randomed_selected_sources = random.sample(sources, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5c79210-db86-482b-a38f-89e586315329",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T07:29:53.367099Z",
     "iopub.status.busy": "2023-05-02T07:29:53.366390Z",
     "iopub.status.idle": "2023-05-02T08:42:32.177845Z",
     "shell.execute_reply": "2023-05-02T08:42:32.167272Z",
     "shell.execute_reply.started": "2023-05-02T07:29:53.367052Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 34895/500000 [33:17<7:23:49, 17.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yoonseonghyeon/Desktop/deeplearning/dataset/prepare_dataset.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yoonseonghyeon/Desktop/deeplearning/dataset/prepare_dataset.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokens \u001b[39m=\u001b[39m encode_from_texts(randomed_selected_sources, tokenizer, block_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, BOS_TOKEN\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m[BOS]\u001b[39;49m\u001b[39m\"\u001b[39;49m, EOS_TOKEN\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m[EOS]\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/yoonseonghyeon/Desktop/deeplearning/dataset/prepare_dataset.ipynb Cell 13\u001b[0m in \u001b[0;36mencode_from_texts\u001b[0;34m(texts, tokenizer, block_size, BOS_TOKEN, EOS_TOKEN)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yoonseonghyeon/Desktop/deeplearning/dataset/prepare_dataset.ipynb#X14sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     temp_tokens \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(np\u001b[39m.\u001b[39mconcatenate((temp_tokens, np\u001b[39m.\u001b[39mones(padding)\u001b[39m*\u001b[39mtokenizer\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m\"\u001b[39m))), (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, block_size\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yoonseonghyeon/Desktop/deeplearning/dataset/prepare_dataset.ipynb#X14sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39m# print(temp_tokens.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yoonseonghyeon/Desktop/deeplearning/dataset/prepare_dataset.ipynb#X14sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     tokens \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate((tokens, temp_tokens), axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(tokens) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m temp_tokens\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yoonseonghyeon/Desktop/deeplearning/dataset/prepare_dataset.ipynb#X14sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tokens\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokens = encode_from_texts(randomed_selected_sources, tokenizer, block_size=128, BOS_TOKEN=\"[BOS]\", EOS_TOKEN=\"[EOS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e3b7c7aa-9d8d-40e2-943c-a8c064e9c9e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T08:44:50.993161Z",
     "iopub.status.busy": "2023-05-02T08:44:50.992129Z",
     "iopub.status.idle": "2023-05-02T08:46:58.335266Z",
     "shell.execute_reply": "2023-05-02T08:46:58.334535Z",
     "shell.execute_reply.started": "2023-05-02T08:44:50.993112Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with gzip.open(\"korean_corpus_dataset_cache.tar.gz\", \"wb\") as f:\n",
    "    np.save(f, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3bc575c4-b107-4a3e-b6ac-8ffd67313a87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T09:35:31.312208Z",
     "iopub.status.busy": "2023-05-02T09:35:31.306299Z",
     "iopub.status.idle": "2023-05-02T09:40:03.657370Z",
     "shell.execute_reply": "2023-05-02T09:40:03.656156Z",
     "shell.execute_reply.started": "2023-05-02T09:35:31.312000Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204886, 129)\n"
     ]
    }
   ],
   "source": [
    "merge_dataset(\"./corpus\", \"corpus.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b76bd5d-6ca2-4147-822c-f7dd753dd109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "9d2639ffb07810fac2cedc92e08a41c0bae42ca785c48ccdb21dd6b5e60bd2fc"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
